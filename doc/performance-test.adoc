ifeval::["{docdir}" != ""]
:imagesdir: D:\hello\doc\image
endif::[]

ifeval::["{docdir}" == ""]
:imagesdir: image
endif::[]

= performance test

This uses POST requests to test the performance of `hello` server. POST requests
to `hello` server are echoed back so the bit rate is bidirectional.  The bit rate does not include
HTTP overhead.

== usage

=== start server

Starting with INFO logging off:

```bash
hello/hello --minloglevel=1
```

Starting with INFO logging off and a response delay of 100 milliseconds:

```bash
hello/hello --minloglevel=1 --echo_request_lag=100
```

==== `hello` parameters:

```
minloglevel: set lowest allowable log level (INFO=0, WARN=1, ERROR=2)
echo_request_lag: milliseconds to wait before sending response (default=0)
```

=== run performance test

==== localhost

```bash
test/perf --number_of_requests=100 --num_connections=10
```

==== remote host

NOTE: Ensure the firewall is open.  Can be verified with: `telnet <ip> <port>`

```bash
test/perf --target_host=192.168.1.107 --target_port=8080 \
  --num_connections=500 --number_of_requests=1000 --payload_size=100 \
  --connect_timeout=5000
```

==== output example

```
I20250130 19:49:24.113564 71890 performance_test.cpp:172] Connecting to: http://192.168.1.107:8080/
I20250130 19:49:34.003443 71890 performance_test.cpp:186] Connected 500 HttpClient's in 9.87364 seconds
I20250130 19:49:34.266191 71890 performance_test.cpp:201]
I20250130 19:49:34.266217 71890 performance_test.cpp:202] sent 1000 HTTP POST requests
I20250130 19:49:34.266224 71890 performance_test.cpp:203] total time       : 0.262065 seconds
I20250130 19:49:34.266250 71890 performance_test.cpp:205] content size     : 100 bytes
I20250130 19:49:34.266273 71890 performance_test.cpp:206] num connections  : 500 connections
I20250130 19:49:34.266281 71890 performance_test.cpp:208] avg request Time : 0.262065 ms
I20250130 19:49:34.266286 71890 performance_test.cpp:209] request rate     : 3815.84 requests/second
I20250130 19:49:34.266290 71890 performance_test.cpp:210] bit rate         : 0.291126 Mbps
```

==== `performance` parameters

NOTE: If this was a *real* performance testing app, there should be an option to set the number of threads.  This uses only one thread.

```
target_host - Host or IP address
target_port - Host port
num_connections - number of HttpClients to use for load testing
number_of_requests - number of requests
payload_size - number of bytes to send in the body of each request
connect_timeout - number of milliseconds to wait for an HttpClient to connect
```

=== ramp script

`ramp.py` was written to simplify performance testing over different numbers of connections, payload sizes, and number of requests.  It ramps one variable, run the performance test multiple times, and reports averages.

This is alpha quality.  The script's values needs to be modified for any given use case.

==== copy script to build directory

from build directory

```bash
cp ../script/ramp.py .
```

==== edit script

Set the script variables as needed then override one of the variables in the ramp loop.

[options="header"]
|===
|script variables   |ramp loop
//-------------
a|
```python
if __name__ == '__main__':
    host = "192.168.1.107"
    port = 8080
    number_of_requests = 1000
    payload_size = 1000
    num_connections = 16
```

a|
```python
    for num_connections in [1, 2, 4, ...]:
```
|===


==== run script

```bash
./ramp.py
```

==== request rate vs payload

[options="header"]
|===
|request rate as a function of connections  |request rate plot
//-------------
a|
----
number of connections=16, request count=10000

payload      min     mean      max
     1      900.4   1086.8   1224.9
    10      794.6    974.9   1116.5
   100      782.2    909.2   1090.8
  1000      556.9    673.6    802.0
 10000      100.6    109.1    121.1
100000       10.9     11.5     12.5
----

a|
image::request-rate-vs-payload.png[]
|===


==== request rate vs number of connections

Ensure that the number of socket connections available on your systems is above your testing values on BOTH the client and server machines.

===== ubuntu

Set limit temporarily in terminal by:

```bash
ulimit -n <max-connections>
```

===== rocky linux

Add these lines:

```bash
* soft nofile 100000
* hard nofile 100000
```

To:

```bash
/etc/security/limits.conf
```

Then restart.  It might be fine to launch a new terminal but that wasn't tested.


[options="header"]
|===
|request rate as a function of connections  |plot of mean values
//-------------
a|
----
payload=10000 bytes, request count=10000

num_connections   min   mean   max
        1          64    68     71
        2          94    99    103
        4         128    138   144
        8         152    157   165
       16         149    159   176
       32         145    159   167
       64         143    154   168
       128        146    153   158
       256        157    162   175
       512        163    172   180
----

a|
image::request-rate-vs-connection-count.png[]
|===


=== factors affecting performance testing

==== number of requests

There can be startup costs that impacts performance numbers.  This is more of a factor when testing against remote servers than local loopback.  Sending 1 request can be a comparable amount of time as sending 100 or even 1000 requests.  This cost is minimized by having a sufficient number of requests.  For example, a 5 second startup cost with 1 request adds five seconds to the average request time but with 1000 requests it adds 5 millisecond to the average.

This could be further mitigated by changing the code to send some initial requests that are not part of the metrics.

It would also be a good idea to measure the individual request times to get an idea of request time distribution.

==== payload size

It's important to test with a payload that is representative of actual payloads.  If you test with 10 bytes and the actual payload is around 16k then your numbers will be off.

==== response validation

An overloaded server can do unexpected things.  Like short circuit its normal processing and send back a warning message.  I forget which tool was used but I was involved in testing where this exact thing happened and we had very misleading request rate numbers.  The performance test verifies the content echoed back.
